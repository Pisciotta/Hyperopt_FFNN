{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation and assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following procedure, you can easily build and test Feed Forward Neural Networks.<br>\n",
    "Hyperparameters are set using Hyperopt Python module.\n",
    "\n",
    "There are some assumptions to remember regarding this particular example:\n",
    "\n",
    "- Data are contained in .csv files. Please note, that if you created the .csv with Excel, it can use \";\" instead of \",\" as value delimiter. In this case replace all \";\" with \",\".\n",
    "- A complete usage of these functions expects to create two folders where any number of .csv files can be placed. The first folder is used to train AND validate the algorithm. The second folder is used to test the accuracy of our model.\n",
    "- The first row of all .csv files must be the column names. Each column corresponds to an input variable. Each row (from the 2nd on) is a set of such input variables values.\n",
    "- It follows that all .csv files must have the same number of columns, but can have a different row number.\n",
    "- By putting different .csv files in the same folder, you let the program merge them as they would be a \"single\" bigger .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from hyperopt import fmin, hp, tpe, STATUS_OK, space_eval, Trials\n",
    "from keras import backend, optimizers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are going to optimize a feed forward neural network. It's a <b>regression</b> problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder = 'train' # we store the .csv training data\n",
    "# in the \"train\" folder. All .csv will be merged together into a single\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model objective function describes our model and needs to be optimize by tuning its hyperparameters.<br/>\n",
    "The hyperparameters are here:\n",
    "- number of hidden layers\n",
    "- units of hidden layers\n",
    "- dropout\n",
    "- loss function\n",
    "- batch size\n",
    "\n",
    "We do not consider the learning rate because we are going to use the Adam Optimizer.<br/>\n",
    "Such parameters are divided into two categories: continuous and discrete values.<br/>\n",
    "As activation function I've chosen the selu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_space = {\n",
    "    'nr_hlayers': hp.choice('nr_hlayers', np.arange(0, 6, 1)), # integers from 0 to 5\n",
    "    'layer_units' : hp.choice('layer_units', np.arange(1, 6, 1)), # integers from 1 to 5\n",
    "    'dropout' :  hp.uniform('dropout', 0, 0.9),\n",
    "    'loss_function': hp.choice('loss_function', ['mean_squared_error',\n",
    "                               'mean_absolute_error',\n",
    "                               'mean_absolute_percentage_error',\n",
    "                               'mean_squared_logarithmic_error'                              \n",
    "                              ]),\n",
    "    'batch_size': hp.choice('batch_size', np.arange(1,66,16))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from comma separated values files in the specified folder\n",
    "# all files are merged together in a unique dataset\n",
    "def get_data(folder):\n",
    "    df_list = []\n",
    "    for f in glob.glob(os.path.join(folder,'*.csv')):\n",
    "        df_list.append(pd.read_csv(f))\n",
    "    df = pd.concat(df_list)\n",
    "    df = df.astype(np.float64)\n",
    "    df = df.dropna() # we filter out rows with non valid values\n",
    "    return df\n",
    "\n",
    "# prepare the inputs and outputs for our ffnn model\n",
    "def feed_in_out(df,output_column_number):\n",
    "    XY = df.to_numpy()\n",
    "    Y = XY[:,output_column_number]\n",
    "    X = np.delete(XY,output_column_number, axis=1)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(training_folder)\n",
    "X,Y = feed_in_out(df,2) # the third column is considered as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hyper_model(X,Y,hyper_params):\n",
    "    model = Sequential() # We sequentially add layers\n",
    "    model.add(Dense(units=X.shape[1]+1, input_dim=X.shape[1], activation='selu')) # input layer\n",
    "    \n",
    "    # hidden layers\n",
    "    for h in np.arange(0, hyper_params['nr_hlayers']):\n",
    "        model.add(Dense(units=hyper_params['layer_units'], activation='selu'))\n",
    "        model.add(Dropout(hyper_params['dropout']))\n",
    "    \n",
    "    model.add(Dense(units=1)) # output layer with linear activation (default)\n",
    "    model.compile(optimizer=\"adam\",loss=hyper_params['loss_function'],metrics=[\"mean_squared_error\"])\n",
    "    history = model.fit(\n",
    "        X,\n",
    "        Y,\n",
    "        batch_size=hyper_params['batch_size'],\n",
    "        validation_split=0.2,\n",
    "        epochs = 20,\n",
    "        shuffle = True,\n",
    "        verbose=0)\n",
    "    \n",
    "    # take the last 8 validation losses, and return their mean value:\n",
    "    return np.mean(history.history['val_mean_squared_error'][-8:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could directly use the Model function to optimize, but it's (in general, maybe not in this case) more modular to embed it in the final objective function which will be optimized.\n",
    "In this way you can define more functions with different outputs and sequentially embed them in the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_fn(hyper_params):\n",
    "    loss = train_hyper_model(X, Y, hyper_params) # X,Y are globally defined!\n",
    "    backend.clear_session() # clear session to avoid models accumulation in memory\n",
    "    return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>: The STATUS_OK value is very important to avoid numerical errors problems produced by some particular set of (hyper)parameters values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's optimize!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Trials, which stores and track the progress, you have the possibility to execute a new optimization process, but starting from previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_trials = Trials()\n",
    "\n",
    "# we can also load trials from file using prickle:\n",
    "f = open('store_trials.pckl', 'rb')\n",
    "keep_trials = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the option trials = keep_trials, if you run again the same cell it will not compute any furter iteration, since it consider the previous ones as completed.\n",
    "For example, if you have done 10 iterations, than you change the iterations to 30 (max_evals = 30) and run the cell again, the optimization will perform 20 iteration (from 11 to 20!).\n",
    "If you want to reset the iteration after each code execution, just move the trials parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 0/5 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3217: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "100%|████████████████████████████████████████████████████| 5/5 [16:38<00:00, 198.27s/it, best loss: 1405.3673982174782]\n",
      "{'batch_size': 1, 'dropout': 0.0025297289561878322, 'layer_units': 1, 'loss_function': 'mean_absolute_error', 'nr_hlayers': 4}\n",
      "number of trials: 75\n",
      "Wall time: 16min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "opt_params = fmin(\n",
    "                fn=hyperopt_fn,\n",
    "                space=hyper_space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=75, # stop searching after 50 iterations\n",
    "                trials = keep_trials\n",
    "                )\n",
    "\n",
    "# store trials in a file\n",
    "f = open('store_trials.pckl', 'wb')\n",
    "pickle.dump(keep_trials, f)\n",
    "f.close()\n",
    "\n",
    "print(space_eval(hyper_space, opt_params))\n",
    "print('number of trials:', len(keep_trials.trials))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
